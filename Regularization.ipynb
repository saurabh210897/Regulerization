{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "276898d8-cda9-4253-8caf-7389e9c77cf1",
   "metadata": {},
   "source": [
    "# Objective: Assess understanding of regularization techniques in deep learning. Evaluate application and comparison of different techniques. Enhance knowledge of regularization's role in improving model generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094348f8-9a32-4c7d-a55b-f6b5ed2820db",
   "metadata": {},
   "source": [
    "Part l: Upderstanding Regularization\n",
    "\n",
    "1. What is regularization in the context of deep learningH Why is it important?\n",
    "2. Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoff.\n",
    "3. Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and their effects on the model.\n",
    "4. Discuss the role of regularization in preventing overfitting and improving the generalization of deep learning models.\n",
    "\n",
    "Part 2: Regularizatiop Techniques\n",
    "\n",
    "5. Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on model training and inference.\n",
    "6. Describe the concept of Early ztopping as a form of regularization. How does it help prevent overfitting during the training process.\n",
    "7. Explain the concept of Batch Normalization and its role as a form of regularization. How does Batch Normalization help in preventing overfitting.\n",
    "\n",
    "Part 3: Applying Regularization\n",
    "\n",
    "8. Implement Dropout regularization in a deep learning model using a framework of your choice. Evaluate its impact on model performance and compare it with a model without Dropoutk\n",
    "9. Discuss the considerations and tradeoffs when choosing the appropriate regularization technique for a given deep learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2026b51a-ad15-4edd-894d-9d793f683c59",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a65ad1-2037-4f4d-8575-e501e274090a",
   "metadata": {},
   "source": [
    "1. What is Regularization in Deep Learning and Its Importance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be2d8088-403c-4e0a-958b-a99dd4b0a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization in the context of deep learning is a set of techniques used to prevent overfitting and improve the generalization performance of neural networks. \n",
    "# Overfitting occurs when a model becomes too complex and fits the training data noise rather than learning the underlying patterns. \n",
    "# Regularization is essential because it helps ensure that the trained model can generalize well to unseen data by controlling the model's complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b270d27c-9a26-416f-8dc8-161354a92e0f",
   "metadata": {},
   "source": [
    "2. Bias-Variance Tradeoff and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cff7961-6e27-49ec-aa1c-ffada71f3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bias-variance tradeoff is a fundamental concept in machine learning. It refers to the balance between two types of errors that a model can make:\n",
    "\n",
    "# Bias (Underfitting): High bias occurs when a model is too simple to capture the underlying patterns in the data. \n",
    "# It results in poor performance both on the training data and unseen data.\n",
    "\n",
    "# Variance (Overfitting): High variance occurs when a model is too complex and fits the training data closely, including noise.\n",
    "# While such a model may perform well on the training data, it often fails to generalize to unseen data.\n",
    "\n",
    "# Regularization helps address this tradeoff by adding a penalty term to the loss function that discourages the model from becoming too complex. \n",
    "# This encourages the model to find a balance between fitting the training data well and not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da194258-7a57-43bc-a1c9-b5d0552f22aa",
   "metadata": {},
   "source": [
    "3. L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cd3ac10-649f-424c-b5b8-b1fa2d2bccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 Regularization (Lasso):\n",
    "\n",
    "# In L1 regularization, a penalty term is added to the loss function that is proportional to the absolute values of the model's weights.\n",
    "# The penalty term is calculated as the sum of the absolute values of the weights, multiplied by a hyperparameter (λ or alpha).\n",
    "# L1 regularization encourages sparsity in the model, meaning it tends to force some weights to become exactly zero, \n",
    "# effectively removing certain features from the model.\n",
    "# L1 regularization can be useful for feature selection, as it identifies and keeps only the most important features.\n",
    "\n",
    "# L2 Regularization (Ridge):\n",
    "\n",
    "# In L2 regularization, a penalty term is added to the loss function that is proportional to the square of the model's weights.\n",
    "# The penalty term is calculated as the sum of the squared values of the weights, multiplied by a hyperparameter (λ or alpha).\n",
    "# L2 regularization encourages the model to have small weights for all features rather than forcing them to become exactly zero.\n",
    "# It helps prevent the model from overemphasizing any particular feature and provides a more balanced approach to regularization.\n",
    "# Differences:\n",
    "\n",
    "# L1 regularization tends to produce sparse models, while L2 regularization produces models with small but non-zero weights for all features.\n",
    "# L1 regularization is more robust to outliers because it doesn't penalize large weights as much as L2 does.\n",
    "# L2 regularization is computationally efficient as its gradient is continuous, whereas L1 regularization can lead to non-differentiability at zero weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6ab17-544c-497b-bcd6-6bd3fac1b3ba",
   "metadata": {},
   "source": [
    "4. Role of Regularization in Preventing Overfitting and Improving Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5156553-988e-4efd-be66-650e5fd655e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization plays a crucial role in preventing overfitting by adding a penalty to the loss function that discourages the model from \n",
    "# fitting the noise in the training data. It helps the model generalize better to unseen data by making it more robust and less prone to capturing random fluctuations.\n",
    "\n",
    "# The benefits of regularization include:\n",
    "\n",
    "# Improved model generalization.\n",
    "# Reduced risk of overfitting, especially in deep and complex neural networks.\n",
    "# Better stability during training, reducing the need for excessive training data.\n",
    "# Enhanced interpretability in the case of L1 regularization, which can highlight important features.\n",
    "# Regularization is an important tool in the deep learning practitioner's toolbox, and its appropriate use can significantly\n",
    "# improve the performance and reliability of neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d5fdef-72ec-47fb-b158-2f37188f8d7e",
   "metadata": {},
   "source": [
    "5. Dropout Regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88c923aa-6a0f-4162-81c1-0d0238e3349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How it Works:\n",
    "# Dropout is a regularization technique that helps reduce overfitting in neural networks. \n",
    "# During training, Dropout randomly sets a fraction of the neurons (units) in a layer to zero for each forward and backward pass.\n",
    "# These randomly dropped neurons do not contribute to the computation of that pass. \n",
    "# The dropout rate is a hyperparameter that determines the probability of a neuron being dropped out, typically ranging from 0.2 to 0.5.\n",
    "\n",
    "# Impact on Model Training and Inference:\n",
    "\n",
    "# During training, Dropout introduces noise and variability into the network, forcing it to be more robust and preventing it from relying too heavily on any\n",
    "# single neuron or feature.\n",
    "# Dropout also effectively creates an ensemble of multiple subnetworks, as different neurons are dropped out in each iteration. \n",
    "# This ensemble effect helps improve generalization.\n",
    "# During inference (when making predictions), Dropout is typically turned off, and the full network is used.\n",
    "# However, the weights of the neurons are scaled down by the dropout rate to ensure that the expected output remains consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732c4190-dd36-433e-8148-524bca49f6c0",
   "metadata": {},
   "source": [
    "6. Early Stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "640137b6-7513-4949-b6a8-c31033ce4f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How it Works:\n",
    "# Early stopping is a form of regularization that involves monitoring the model's performance on a validation dataset during training.\n",
    "# The training process is stopped when the model's performance on the validation set starts deteriorating,\n",
    "# even if the performance on the training set continues to improve. This is typically done by tracking a specific metric (e.g., validation loss or accuracy) \n",
    "# and comparing it to previous values.\n",
    "\n",
    "# Preventing Overfitting:\n",
    "# Early stopping helps prevent overfitting by monitoring the point at which the model starts to overfit the training data. \n",
    "# It stops training before the model's performance on the validation set degrades, ensuring that the model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a812a6-bd43-4a2b-bd15-4aeb2da0d9cf",
   "metadata": {},
   "source": [
    "7. Batch Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ed1a21-ef09-4f2f-afc3-f4c39dc399f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How it Works:\n",
    "# Batch Normalization (BatchNorm) is a technique that normalizes the input of each layer within a mini-batch. \n",
    "# It standardizes the mean and variance of each feature to have a specific distribution (typically mean of 0 and variance of 1).\n",
    "# BatchNorm introduces learnable scaling and shifting parameters to restore the representation power of the network.\n",
    "\n",
    "# Role as Regularization:\n",
    "\n",
    "# BatchNorm acts as a form of regularization by reducing internal covariate shift, which is a change in the distribution of hidden activations during training. \n",
    "# This helps stabilize and speed up training.\n",
    "# By reducing covariate shift, BatchNorm allows for the use of higher learning rates, which can speed up convergence and make the training process more robust.\n",
    "# It reduces the reliance on specific weight initialization schemes and helps prevent exploding or vanishing gradients.\n",
    "# Batch Normalization is an effective regularization technique that not only stabilizes training but also improves the generalization of deep neural networks by making \n",
    "# them less sensitive to variations in the input data distribution.\n",
    "\n",
    "# In summary, regularization techniques like Dropout, Early Stopping, and Batch Normalization are essential tools in preventing overfitting and improving \n",
    "# the generalization of deep learning models. They help control the model's complexity, stabilize training, and ensure that the trained network performs \n",
    "# well on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4a7eb8c-b98a-4b44-bf0b-cf204887be96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-2.3.7-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.23.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.4/181.4 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.13)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.23.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.58.0 keras-2.13.1 libclang-16.0.6 markdown-3.4.4 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.3.0 werkzeug-2.3.7 wrapt-1.15.0\n",
      "Requirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (2.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow \n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bab5e8a0-94ba-41e9-b9a3-558a9881e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf066106-7bd4-4c1e-a0e3-cdf9f1b7bcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5648db2a-8f6f-4a49-9c47-ec38440c6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type of X_train_full: uint8,\n",
      " shape of X_train_full: (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(f\"data type of X_train_full: {X_train_full.dtype},\\n shape of X_train_full: {X_train_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9a6e812-f5fc-48cc-8f68-ce6ddce21929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5739ca7d-a5bb-4d06-9ffc-4f140930248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data['quality'] = label_encoder.fit_transform(data['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58bf671d-1f97-428b-b006-1874fc1ef786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "739c0d6b-ff8c-4976-9de8-aafbfb29e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a validation data set from the full training data \n",
    "# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range\n",
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# scale the test set as well\n",
    "X_test = X_test / 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f05cc8b8-84e9-4659-8e68-d435d7d2cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e40cb7c9-83df-45f8-b3f5-91e2201123e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_full[5000:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb8a87b8-1dad-4f06-b8a2-eb73da4ceb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network model with Dropout\n",
    "model_with_dropout = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),  # Example input shape for image data\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),  # Dropout layer with a dropout rate of 0.5 (adjust as needed)\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),  # Another Dropout layer\n",
    "    keras.layers.Dense(10, activation='softmax')  # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee5cbd35-92f2-4bbe-967d-c627751ac368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_with_dropout.compile(optimizer='adam',\n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e08f4d24-d20e-49de-96a0-32e1ded37a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.6713 - accuracy: 0.7895 - val_loss: 0.2119 - val_accuracy: 0.9435\n",
      "Epoch 2/10\n",
      "1375/1375 [==============================] - 5s 3ms/step - loss: 0.3648 - accuracy: 0.8976 - val_loss: 0.1674 - val_accuracy: 0.9530\n",
      "Epoch 3/10\n",
      "1375/1375 [==============================] - 5s 3ms/step - loss: 0.3047 - accuracy: 0.9132 - val_loss: 0.1453 - val_accuracy: 0.9615\n",
      "Epoch 4/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2644 - accuracy: 0.9249 - val_loss: 0.1354 - val_accuracy: 0.9625\n",
      "Epoch 5/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2442 - accuracy: 0.9307 - val_loss: 0.1293 - val_accuracy: 0.9642\n",
      "Epoch 6/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2291 - accuracy: 0.9350 - val_loss: 0.1253 - val_accuracy: 0.9652\n",
      "Epoch 7/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2176 - accuracy: 0.9387 - val_loss: 0.1245 - val_accuracy: 0.9687\n",
      "Epoch 8/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2060 - accuracy: 0.9400 - val_loss: 0.1158 - val_accuracy: 0.9701\n",
      "Epoch 9/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.1978 - accuracy: 0.9432 - val_loss: 0.1121 - val_accuracy: 0.9703\n",
      "Epoch 10/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.1889 - accuracy: 0.9451 - val_loss: 0.1095 - val_accuracy: 0.9717\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history_with_dropout = model_with_dropout.fit(X_train, y_train, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a103e771-3e00-494c-a8b0-f8a6a6bea831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network model without Dropout for comparison\n",
    "model_without_dropout = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa5ed051-6f12-4e24-b235-91e915d34a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model without Dropout\n",
    "model_without_dropout.compile(optimizer='adam',\n",
    "                             loss='sparse_categorical_crossentropy',\n",
    "                             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cf5dfed-6e0e-4abc-acb8-531713c3ecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1375/1375 [==============================] - 5s 3ms/step - loss: 0.2795 - accuracy: 0.9187 - val_loss: 0.1375 - val_accuracy: 0.9595\n",
      "Epoch 2/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.1148 - accuracy: 0.9648 - val_loss: 0.1047 - val_accuracy: 0.9687\n",
      "Epoch 3/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.0782 - accuracy: 0.9745 - val_loss: 0.1032 - val_accuracy: 0.9681\n",
      "Epoch 4/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.0583 - accuracy: 0.9812 - val_loss: 0.1051 - val_accuracy: 0.9684\n",
      "Epoch 5/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.0462 - accuracy: 0.9845 - val_loss: 0.0942 - val_accuracy: 0.9725\n",
      "Epoch 6/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.0338 - accuracy: 0.9895 - val_loss: 0.1035 - val_accuracy: 0.9722\n",
      "Epoch 7/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.0294 - accuracy: 0.9901 - val_loss: 0.1114 - val_accuracy: 0.9725\n",
      "Epoch 8/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.0247 - accuracy: 0.9918 - val_loss: 0.1038 - val_accuracy: 0.9726\n",
      "Epoch 9/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.0216 - accuracy: 0.9930 - val_loss: 0.1076 - val_accuracy: 0.9741\n",
      "Epoch 10/10\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.0207 - accuracy: 0.9933 - val_loss: 0.1162 - val_accuracy: 0.9735\n"
     ]
    }
   ],
   "source": [
    "history_without_dropout = model_without_dropout.fit(X_train, y_train, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db393765-093b-49c0-8190-f35442c268f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9685\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9728\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and compare the two models\n",
    "test_loss_dropout, test_acc_dropout = model_with_dropout.evaluate(X_test, y_test)\n",
    "test_loss_no_dropout, test_acc_no_dropout = model_without_dropout.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee77a9d2-1707-4f84-82b3-858ecb27125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with Dropout - Test Accuracy: 0.968500018119812\n",
      "Model without Dropout - Test Accuracy: 0.9728000164031982\n"
     ]
    }
   ],
   "source": [
    "print(\"Model with Dropout - Test Accuracy:\", test_acc_dropout)\n",
    "print(\"Model without Dropout - Test Accuracy:\", test_acc_no_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a441b91d-4eba-4908-bc7e-1566b07393ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considerations and Tradeoffs when Choosing Regularization Techniques:\n",
    "\n",
    "# Type of Data: The choice of regularization depends on the type of data and problem. For example, Dropout is commonly used in image classification, \n",
    "# while sequence data may benefit from techniques like recurrent dropout.\n",
    "\n",
    "# Model Complexity: The complexity of your model and its tendency to overfit should guide your choice of regularization. \n",
    "# More complex models often require stronger regularization.\n",
    "\n",
    "# Computational Resources: Some regularization techniques, like Dropout, introduce randomness during training, which may require longer training times \n",
    "# and more computational resources.\n",
    "\n",
    "# Hyperparameter Tuning: The dropout rate (in the case of Dropout), regularization strength, and other hyperparameters should be tuned using techniques\n",
    "# like cross-validation to find the best values for your specific problem.\n",
    "\n",
    "# Other Regularization Techniques: Consider other techniques like L1/L2 regularization, Batch Normalization, and early stopping,\n",
    "# and how they complement or substitute for each other.\n",
    "\n",
    "# Domain Knowledge: Understanding the nature of your data and problem can guide your choice of regularization. \n",
    "# For example, if you know that certain features are less relevant, L1 regularization may be suitable.\n",
    "\n",
    "# Validation Performance: Regularization techniques should be selected based on their impact on validation performance. \n",
    "# Monitor training and validation loss and accuracy to determine if a model is overfitting and whether regularization is needed.\n",
    "\n",
    "# Experimentation: Experiment with different regularization techniques and hyperparameter settings to find the combination that works best for your specific \n",
    "# deep learning task.\n",
    "\n",
    "# The choice of regularization technique should be made thoughtfully, considering the tradeoffs and characteristics of the problem and data at hand.\n",
    "# Regularization is a crucial part of training robust and generalizable deep learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
